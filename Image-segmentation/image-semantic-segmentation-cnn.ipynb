{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing the modules","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport requests\n\nimport matplotlib.pyplot as plt\n\n# torchvision related imports.\nimport torchvision.transforms.functional as F\nfrom torchvision.io import read_image\nfrom torchvision.utils import draw_bounding_boxes\nfrom torchvision.utils import make_grid\n\n# models and transforms\nfrom torchvision.transforms.functional import convert_image_dtype\nfrom torchvision.models.segmentation import fcn_resnet50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T10:08:51.227216Z","iopub.execute_input":"2025-06-19T10:08:51.227522Z","iopub.status.idle":"2025-06-19T10:08:51.234209Z","shell.execute_reply.started":"2025-06-19T10:08:51.227502Z","shell.execute_reply":"2025-06-19T10:08:51.233115Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining the utilities","metadata":{}},{"cell_type":"code","source":"## utilities for multiple images.\ndef img_show(images):\n    if not isinstance(images, list):\n        # generalize cast images to list\n        images = [images]\n    fig, axis = plt.subplots(ncols=len(images), squeeze=False)\n    for i, image in enumerate(images):\n        image = image.detach() #detach from current DAG, no gradient.\n        image = F.to_pil_image(image)\n        axis[0, i].imshow(np.asarray(image))\n        axis[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T09:52:02.810549Z","iopub.execute_input":"2025-06-19T09:52:02.810973Z","iopub.status.idle":"2025-06-19T09:52:02.818756Z","shell.execute_reply.started":"2025-06-19T09:52:02.810949Z","shell.execute_reply":"2025-06-19T09:52:02.817760Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Getting the image to be segmented","metadata":{}},{"cell_type":"code","source":"url = \"https://raw.githubusercontent.com/Apress/computer-vision-projects-with-pytorch/main/chapter4/semantic_example_highway.jpg\"\n\n# Saving locally for read_image to use\nwith open(\"semantic_example_highway.jpg\", \"wb\") as f:\n    f.write(requests.get(url).content)\n\n#torchvision.io.read_image (returns a Tensor)\nimg1 = read_image(\"semantic_example_highway.jpg\")  # [C, H, W]\nprint(img_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T10:01:47.928600Z","iopub.execute_input":"2025-06-19T10:01:47.928929Z","iopub.status.idle":"2025-06-19T10:01:47.957689Z","shell.execute_reply.started":"2025-06-19T10:01:47.928906Z","shell.execute_reply":"2025-06-19T10:01:47.956678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"box_car = torch.tensor([[170, 70, 220, 120]], dtype=torch.float) ## (xmin,ymin,ymax)\ncolors = [\"blue\"]\ncheck_box = draw_bounding_boxes(img1, box_car, colors=colors, width=2)\nimg_show(check_box)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T10:05:19.268732Z","iopub.execute_input":"2025-06-19T10:05:19.269196Z","iopub.status.idle":"2025-06-19T10:05:19.685529Z","shell.execute_reply.started":"2025-06-19T10:05:19.269145Z","shell.execute_reply":"2025-06-19T10:05:19.684360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# batch for images.\nbatch_imgs = torch.stack([img1])\nbatch_torch = convert_image_dtype(batch_imgs, dtype=torch.float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T10:09:02.255863Z","iopub.execute_input":"2025-06-19T10:09:02.256211Z","iopub.status.idle":"2025-06-19T10:09:02.263206Z","shell.execute_reply.started":"2025-06-19T10:09:02.256184Z","shell.execute_reply":"2025-06-19T10:09:02.262308Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the model for evaluation.","metadata":{}},{"cell_type":"code","source":"model = fcn_resnet50(pretrained=True, progress=False)\n\n# switching on evaluation mode.\nmodel = model.eval()\n\n# standard normalizing based on train config.\nnormalized_batch_torch = F.normalize(batch_torch, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\nresult = model(normalized_batch_torch)['out']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T10:15:05.171011Z","iopub.execute_input":"2025-06-19T10:15:05.171952Z","iopub.status.idle":"2025-06-19T10:15:09.038459Z","shell.execute_reply.started":"2025-06-19T10:15:05.171839Z","shell.execute_reply":"2025-06-19T10:15:09.037315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Passing the image through the model.","metadata":{}},{"cell_type":"code","source":"classes = [\n    '__background__', 'aeroplane', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n'horse', 'motorbike','person', 'pottedplant', 'sheep', 'sofa', 'train','tvmonitor'\n]\n\nclass_to_idx = {cls: idx for (idx, cls) in enumerate(classes)}\n\nnormalized_out_masks = torch.nn.functional.softmax(result, dim=1)\n\ncar_mask = [\n    normalized_out_masks[img_idx, class_to_idx[cls]]\n    for img_idx in range(batch_torch.shape[0])\n    for cls in ('car', 'pottedplant', 'bus')\n]\n\nimg_show(car_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T10:20:16.551736Z","iopub.execute_input":"2025-06-19T10:20:16.552031Z","iopub.status.idle":"2025-06-19T10:20:16.742131Z","shell.execute_reply.started":"2025-06-19T10:20:16.552010Z","shell.execute_reply":"2025-06-19T10:20:16.741207Z"}},"outputs":[],"execution_count":null}]}